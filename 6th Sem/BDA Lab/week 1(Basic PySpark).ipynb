{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79c841fe",
   "metadata": {},
   "source": [
    "# Q1.\n",
    "# Write a PySpark program to square set of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aeda7a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb678304",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.config(\"spark.driver.memory\", \"4g\").appName('square').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f30ddee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sub1: integer (nullable = true)\n",
      " |-- sub2: integer (nullable = true)\n",
      " |-- sub3: integer (nullable = true)\n",
      "\n",
      "+----+----+----+\n",
      "|sub1|sub2|sub3|\n",
      "+----+----+----+\n",
      "|   1|   2|   3|\n",
      "|   4|   5|   6|\n",
      "|   7|   8|   9|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "data=[[1, 2, 3],[4,5,6],[7,8,9]]\n",
    "columns=[\"sub1\",\"sub2\",\"sub3\"]\n",
    "schema = StructType([ \n",
    "    StructField('sub1', \n",
    "                IntegerType(), True), \n",
    "    StructField('sub2', \n",
    "                IntegerType(), True), \n",
    "    StructField('sub3', \n",
    "                IntegerType(),True)\n",
    "    \n",
    "]) \n",
    "df = spark.createDataFrame(data, schema=schema )\n",
    "df.printSchema() # It will print the Schema\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "122129fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.10/site-packages (2.1.1)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.1.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.local/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.local/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in ./.local/lib/python3.10/site-packages (from pandas) (1.26.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.1.1\n",
      "    Uninstalling pandas-2.1.1:\n",
      "      Successfully uninstalled pandas-2.1.1\n",
      "Successfully installed pandas-2.1.4\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "84ba54f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16, 25]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"SquareIntegers\").getOrCreate()\n",
    "\n",
    "# Square the integers and collect the results\n",
    "result = spark.sparkContext.parallelize([1, 2, 3, 4, 5]).map(lambda x: x ** 2).collect()\n",
    "\n",
    "# Print the squared integers\n",
    "print(result)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "18d61cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lplab/anaconda3/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number: 15\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session and find the maximum\n",
    "print(\"Maximum number:\", SparkSession.builder.appName(\"MaxOfNumbers\").getOrCreate().sparkContext.parallelize([10, 5, 8, 15, 3]).max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4592bbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average: 8.2\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session and find the average\n",
    "print(\"Average:\", SparkSession.builder.appName(\"AverageOfNumbers\").getOrCreate().sparkContext.parallelize([10, 5, 8, 15, 3]).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7f3e86bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+----+----+----+----+----+----+----+----+\n",
      "|Rank|Player| _c2| _c3| _c4| _c5| _c6| _c7| _c8| _c9|\n",
      "+----+------+----+----+----+----+----+----+----+----+\n",
      "|   1|     A|null|null|null|null|null|null|null|null|\n",
      "|   2|     B|null|null|null|null|null|null|null|null|\n",
      "|   3|     C|null|null|null|null|null|null|null|null|\n",
      "|null|  null|null|null|null|null|null|null|null|null|\n",
      "|null|  null|null|null|null|null|null|null|null|null|\n",
      "|null|  null|null|null|null|null|null|null|null|null|\n",
      "|null|  null|null|null|null|null|null|null|null|null|\n",
      "|null|  null|null|null|null|null|null|null|null|null|\n",
      "|null|  null|null|null|null|null|null|null|null|null|\n",
      "+----+------+----+----+----+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Read CSV into DataFrame\n",
    "df = SparkSession.builder.appName(\"ReadCSV\").getOrCreate().read.csv(\"/home/lplab/Desktop/lab1.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "be98e090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+----+----+----+----+----+----+----+----+\n",
      "|Rank|Player| _c2| _c3| _c4| _c5| _c6| _c7| _c8| _c9|\n",
      "+----+------+----+----+----+----+----+----+----+----+\n",
      "|   1|     A|null|null|null|null|null|null|null|null|\n",
      "|   2|     B|null|null|null|null|null|null|null|null|\n",
      "+----+------+----+----+----+----+----+----+----+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Read CSV into DataFrame and display the first 2 rows and schema\n",
    "SparkSession.builder.appName(\"DisplayDataFrame\").getOrCreate().read.csv(\"/home/lplab/Desktop/lab1.csv\", header=True, inferSchema=True).show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "da4ce8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|summary|Rank|\n",
      "+-------+----+\n",
      "|  count|   3|\n",
      "|   mean| 2.0|\n",
      "| stddev| 1.0|\n",
      "|    min|   1|\n",
      "|    max|   3|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Read CSV into DataFrame and calculate summary statistics for a specific column\n",
    "SparkSession.builder.appName(\"ColumnSummary\").getOrCreate().read.csv(\"/home/lplab/Desktop/lab1.csv\", header=True, inferSchema=True).select(\"Rank\").describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc76b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
