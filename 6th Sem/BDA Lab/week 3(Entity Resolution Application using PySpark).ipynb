{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c0d4685",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lplab/anaconda3/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+-------------+-------------------+\n",
      "|Name      |Address        |NameTokens   |AddressTokens      |\n",
      "+----------+---------------+-------------+-------------------+\n",
      "|John Doe  |123 Main St    |[john, doe]  |[123, main, st]    |\n",
      "|john do   |123 main street|[john, do]   |[123, main, street]|\n",
      "|Jane Smith|456 Oak Ave    |[jane, smith]|[456, oak, ave]    |\n",
      "+----------+---------------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, split\n",
    "\n",
    "spark = SparkSession.builder.appName(\"EntityResolution\").getOrCreate()\n",
    "\n",
    "data = [(\"John Doe\", \"123 Main St\"), (\"john do\", \"123 main street\"), (\"Jane Smith\", \"456 Oak Ave\")]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Address\"])\n",
    "\n",
    "df_cleaned = df.withColumn(\"NameTokens\", split(lower(col(\"Name\")), \" \")).withColumn(\"AddressTokens\", split(lower(col(\"Address\")), \" \"))\n",
    "\n",
    "df_cleaned.show(truncate=False)\n",
    "   \n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "342876f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lplab/anaconda3/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+\n",
      "|Name       |Name      |Similarity|\n",
      "+-----------+----------+----------+\n",
      "|Jane Smith |John Doe  |0.2       |\n",
      "|Bob Johnson|John Doe  |0.2       |\n",
      "|Bob Johnson|Jane Smith|0.2       |\n",
      "+-----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SimilarityScores\").getOrCreate()\n",
    "\n",
    "data = [(\"John Doe\", [\"apple\", \"orange\", \"banana\"]),\n",
    "        (\"Jane Smith\", [\"banana\", \"grape\", \"kiwi\"]),\n",
    "        (\"Bob Johnson\", [\"orange\", \"kiwi\", \"pear\"])]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Tokens\"])\n",
    "\n",
    "# Convert the \"Tokens\" column to sets\n",
    "df = df.withColumn(\"Tokens\", col(\"Tokens\").cast(\"array<string>\").alias(\"Tokens\"))\n",
    "\n",
    "similarity_udf = lambda set1, set2: len(set(set1).intersection(set(set2))) / len(set(set1).union(set(set2))) if len(set(set1).union(set(set2))) > 0 else 0.0\n",
    "jaccard_similarity_udf = spark.udf.register(\"jaccard_similarity\", similarity_udf, FloatType())\n",
    "\n",
    "pairwise_similarity = (df.alias(\"df1\").crossJoin(df.alias(\"df2\")).filter(col(\"df1.Name\") < col(\"df2.Name\"))\n",
    "                      .withColumn(\"Similarity\", jaccard_similarity_udf(\"df1.Tokens\", \"df2.Tokens\"))\n",
    "                      .select(\"df1.Name\", \"df2.Name\", \"Similarity\"))\n",
    "\n",
    "pairwise_similarity.show(truncate=False)\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8187641",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lplab/anaconda3/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------+----------+-----------------------+----------+\n",
      "|Name       |Tokens               |Name      |Tokens                 |Similarity|\n",
      "+-----------+---------------------+----------+-----------------------+----------+\n",
      "|Jane Smith |[banana, grape, kiwi]|John Doe  |[apple, orange, banana]|0.2       |\n",
      "|Bob Johnson|[orange, kiwi, pear] |John Doe  |[apple, orange, banana]|0.2       |\n",
      "|Bob Johnson|[orange, kiwi, pear] |Jane Smith|[banana, grape, kiwi]  |0.2       |\n",
      "+-----------+---------------------+----------+-----------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SimilarityScores\").getOrCreate()\n",
    "\n",
    "data = [(\"John Doe\", [\"apple\", \"orange\", \"banana\"]),\n",
    "        (\"Jane Smith\", [\"banana\", \"grape\", \"kiwi\"]),\n",
    "        (\"Bob Johnson\", [\"orange\", \"kiwi\", \"pear\"])]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Tokens\"])\n",
    "\n",
    "# Convert the \"Tokens\" column to sets\n",
    "df = df.withColumn(\"Tokens\", col(\"Tokens\").cast(\"array<string>\").alias(\"Tokens\"))\n",
    "\n",
    "similarity_udf = lambda set1, set2: len(set(set1).intersection(set(set2))) / len(set(set1).union(set(set2))) if len(set(set1).union(set(set2))) > 0 else 0.0\n",
    "jaccard_similarity_udf = spark.udf.register(\"jaccard_similarity\", similarity_udf, FloatType())\n",
    "\n",
    "pairwise_similarity = (df.alias(\"df1\").crossJoin(df.alias(\"df2\")).filter(col(\"df1.Name\") < col(\"df2.Name\"))\n",
    "                      .withColumn(\"Similarity\", jaccard_similarity_udf(\"df1.Tokens\", \"df2.Tokens\"))\n",
    "                      .select(\"df1.Name\", \"df1.Tokens\", \"df2.Name\", \"df2.Tokens\", \"Similarity\"))\n",
    "\n",
    "pairwise_similarity.show(truncate=False)\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23e81d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+---+---+---------+------+--------+\n",
      "|Name       |TP |FP |FN |Precision|Recall|F1_Score|\n",
      "+-----------+---+---+---+---------+------+--------+\n",
      "|John Doe   |1  |0  |0  |1.0      |1.0   |1.0     |\n",
      "|Jane Smith |1  |0  |0  |1.0      |1.0   |1.0     |\n",
      "|Bob Johnson|0  |1  |0  |0.0      |null  |null    |\n",
      "+-----------+---+---+---+---------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"EntityResolutionEvaluation\").getOrCreate()\n",
    "\n",
    "# Sample data (replace with your actual data)\n",
    "ground_truth = [(\"John Doe\", \"john.doe@gmail.com\", \"123 Main St\"),\n",
    "                (\"Jane Smith\", \"jane.smith@gmail.com\", \"456 Oak Ave\")]\n",
    "\n",
    "predicted = [(\"John Doe\", \"john.doe@gmail.com\", \"123 Main Street\"),\n",
    "              (\"Jane Smith\", \"jane.smith@yahoo.com\", \"456 Oak Avenue\"),\n",
    "              (\"Bob Johnson\", \"bob.johnson@gmail.com\", \"789 Pine St\")]\n",
    "\n",
    "ground_truth_df = spark.createDataFrame(ground_truth, [\"Name\", \"Email\", \"Address\"])\n",
    "predicted_df = spark.createDataFrame(predicted, [\"Name\", \"Email\", \"Address\"])\n",
    "\n",
    "# Join predicted and ground truth data\n",
    "joined_df = predicted_df.alias(\"p\").join(ground_truth_df.alias(\"g\"), \"Name\", \"left_outer\")\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "metrics = joined_df.groupBy(\"Name\").agg(\n",
    "    sum((col(\"p.Email\").isNotNull() & col(\"g.Email\").isNotNull()).cast(\"int\")).alias(\"TP\"),\n",
    "    sum((col(\"p.Email\").isNotNull() & col(\"g.Email\").isNull()).cast(\"int\")).alias(\"FP\"),\n",
    "    sum((col(\"p.Email\").isNull() & col(\"g.Email\").isNotNull()).cast(\"int\")).alias(\"FN\")\n",
    ")\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "metrics = metrics.withColumn(\n",
    "    \"Precision\", col(\"TP\") / (col(\"TP\") + col(\"FP\"))\n",
    ").withColumn(\n",
    "    \"Recall\", col(\"TP\") / (col(\"TP\") + col(\"FN\"))\n",
    ").withColumn(\n",
    "    \"F1_Score\", 2 * (col(\"Precision\") * col(\"Recall\")) / (col(\"Precision\") + col(\"Recall\"))\n",
    ")\n",
    "\n",
    "# Display evaluation metrics\n",
    "metrics.show(truncate=False)\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cdbb3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
